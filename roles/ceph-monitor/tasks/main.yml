---
# Ceph Monitor setup tasks - Simplified for Fedora compatibility

- name: Install Ceph prerequisites
  dnf:
    name:
      - ceph
      - ceph-mon
      - ceph-mgr
      - ceph-osd
      - python3-ceph-common
      - python3-rbd
      - ceph-common
    state: present

- name: Create Ceph base directory with proper permissions
  file:
    path: /var/lib/ceph
    state: directory
    mode: "0755"
    owner: ceph
    group: ceph
    recurse: yes

- name: Create Ceph subdirectories
  file:
    path: "{{ item }}"
    state: directory
    mode: "0755"
    owner: ceph
    group: ceph
  loop:
    - /etc/ceph
    - /var/lib/ceph/mon
    - /var/lib/ceph/osd
    - /var/lib/ceph/mds
    - /var/log/ceph

- name: Check if Ceph is already configured
  stat:
    path: /etc/ceph/ceph.conf
  register: ceph_conf_stat

# IDEMPOTENCY SAFEGUARD:
# Only initialize the Ceph monitor if:
# 1. Monitor filesystem does not exist (/var/lib/ceph/mon/ceph-<hostname>)
# 2. This is the first (primary) monitor node
# 3. The cluster is not already healthy (prevents re-initialization on retry runs)
# This prevents accidentally wiping an existing cluster when the playbook is re-run.

- name: Check if Ceph monitor is already initialized
  stat:
    path: "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
  register: ceph_mon_fs_stat

- name: Check if Ceph cluster is already healthy
  shell: |
    timeout 5 ceph status 2>/dev/null | grep -q "mon: " && echo "healthy" || echo "not-ready"
  register: ceph_cluster_check
  changed_when: false
  ignore_errors: yes

- name: Set monitor initialization flag
  set_fact:
    should_init_monitor: "{{ not ceph_mon_fs_stat.stat.exists and inventory_hostname == groups['ceph_monitors'][0] and ceph_cluster_check.stdout != 'healthy' }}"

- name: Debug - Show monitor filesystem check result
  debug:
    msg: |
      Monitor FS check:
      - Hostname: {{ inventory_hostname }}
      - First monitor: {{ groups['ceph_monitors'][0] }}
      - Is first monitor: {{ inventory_hostname == groups['ceph_monitors'][0] }}
      - FS exists: {{ ceph_mon_fs_stat.stat.exists }}
      - Cluster healthy: {{ ceph_cluster_check.stdout | default('unknown') }}
      - Should initialize: {{ should_init_monitor }}

- name: Generate Ceph FSID
  shell: |
    uuidgen
  register: ceph_fsid
  changed_when: false
  when: should_init_monitor

- name: Debug - FSID generation
  debug:
    msg: |
      FSID Generation:
      - Changed: {{ ceph_fsid.changed | default('N/A') }}
      - RC: {{ ceph_fsid.rc | default('N/A') }}
      - FSID: {{ ceph_fsid.stdout | default('EMPTY') }}
  when: should_init_monitor

- name: Set Ceph FSID fact
  set_fact:
    ceph_cluster_fsid: "{{ ceph_fsid.stdout | trim }}"
  when: should_init_monitor

- name: Create Ceph configuration file
  template:
    src: ceph.conf.j2
    dest: /etc/ceph/ceph.conf
    mode: "0644"
  vars:
    mon_host: "{% for host in groups['ceph_monitors'] %}{{ hostvars[host].ansible_default_ipv4.address }}:6789{% if not loop.last %},{% endif %}{% endfor %}"
    ceph_cluster_fsid: "{{ hostvars[groups['ceph_monitors'][0]]['ceph_cluster_fsid'] | default('') }}"
  when: should_init_monitor
  register: ceph_conf_result

- name: Display ceph.conf content (for debugging)
  shell: cat /etc/ceph/ceph.conf
  register: ceph_conf_content
  changed_when: false
  when: not ceph_conf_stat.stat.exists and inventory_hostname == groups['ceph_monitors'][0]
  ignore_errors: yes

- name: Show ceph.conf
  debug:
    msg: "{{ ceph_conf_content.stdout }}"
  when: not ceph_conf_stat.stat.exists and inventory_hostname == groups['ceph_monitors'][0]

- name: Create Ceph keyring directory
  file:
    path: /etc/ceph/keyring.d
    state: directory
    mode: "0755"
    owner: ceph
    group: ceph

- name: Generate Ceph monitor key
  shell: |
    ceph-authtool --create-keyring /tmp/ceph.mon.keyring \
      --gen-key -n mon. \
      --cap mon 'allow *'
  args:
    creates: /tmp/ceph.mon.keyring
  when: should_init_monitor
  register: mon_key_gen

- name: Debug - Monitor key generation
  shell: |
    ls -la /tmp/ceph.mon.keyring 2>&1 || echo "File does not exist"
  register: mon_key_check
  changed_when: false
  when: should_init_monitor

- name: Show monitor key generation result
  debug:
    msg: "{{ mon_key_check.stdout }}"
  when: should_init_monitor

- name: Generate Ceph admin key in temporary location
  shell: |
    ceph-authtool --create-keyring /tmp/ceph.client.admin.keyring \
      --gen-key -n client.admin \
      --cap mon 'allow *' \
      --cap osd 'allow *' \
      --cap mds 'allow *' \
      --cap mgr 'allow *'
  args:
    creates: /tmp/ceph.client.admin.keyring
  when: should_init_monitor
  register: admin_key_gen

- name: Debug - Admin key generation
  shell: |
    ls -la /tmp/ceph.client.admin.keyring 2>&1 || echo "File does not exist"
  register: admin_key_check
  changed_when: false
  when: should_init_monitor

- name: Show admin key generation result
  debug:
    msg: "{{ admin_key_check.stdout }}"
  when: should_init_monitor

- name: Import admin key into monitor keyring (BEFORE mkfs)
  shell: |
    ceph-authtool /tmp/ceph.mon.keyring --import-keyring /tmp/ceph.client.admin.keyring
  when: should_init_monitor
  ignore_errors: yes

- name: Check if admin keyring exists in temp location
  stat:
    path: /tmp/ceph.client.admin.keyring
  register: admin_keyring_stat
  when: not ceph_conf_stat.stat.exists and inventory_hostname == groups['ceph_monitors'][0]

- name: Copy admin keyring to final location
  copy:
    src: /tmp/ceph.client.admin.keyring
    dest: /etc/ceph/ceph.client.admin.keyring
    remote_src: yes
    mode: "0600"
    owner: ceph
    group: ceph
  when:
    - not ceph_mon_fs_stat.stat.exists
    - inventory_hostname == groups['ceph_monitors'][0]
    - admin_keyring_stat.stat.exists | default(false)

- name: Generate monmap
  shell: |
    monmaptool --create {% for host in groups['ceph_monitors'] %} \
      --add {{ hostvars[host]['inventory_hostname'] }} {{ hostvars[host]['ansible_default_ipv4']['address'] }}:6789{% endfor %} \
      /tmp/monmap
  args:
    creates: /tmp/monmap
  when: should_init_monitor

- name: Clean up failed monitor setup (if it exists)
  shell: |
    if [ -d "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}" ]; then
      echo "Cleaning up failed monitor directory..."
      rm -rf "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
    fi
  ignore_errors: yes
  when: should_init_monitor

- name: Initialize Ceph monitor with detailed output
  shell: |
    echo "Starting monitor initialization..."
    echo "Monmap location: /tmp/monmap"
    echo "Keyring location: /tmp/ceph.mon.keyring"
    echo "Target directory: /var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
    echo "Current user: $(whoami)"
    echo "UID/GID: $(id)"

    ls -la /tmp/monmap /tmp/ceph.mon.keyring 2>&1

    echo "Creating monitor directory..."
    mkdir -p "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
    ls -la /var/lib/ceph/mon/ 2>&1

    echo "Running ceph-mon mkfs..."
    ceph-mon --mkfs -i {{ inventory_hostname }} \
      --monmap /tmp/monmap \
      --keyring /tmp/ceph.mon.keyring 2>&1

    echo "Monitor initialization completed"
    ls -la "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}/" 2>&1
  args:
    creates: "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}/store.db"
  when: should_init_monitor
  register: mon_mkfs
  failed_when: mon_mkfs.rc != 0

- name: Display monitor initialization output
  debug:
    msg: "{{ mon_mkfs.stdout }}"
  when: should_init_monitor

- name: Fix permissions on monitor directory
  file:
    path: "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
    owner: ceph
    group: ceph
    recurse: yes

- name: Fix Ceph directory permissions before starting service
  shell: |
    chmod -R 755 /var/lib/ceph
    chmod -R 755 /var/log/ceph
    chown -R ceph:ceph /var/lib/ceph /var/log/ceph
  ignore_errors: yes

- name: Verify monitor filesystem exists
  stat:
    path: "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
  register: mon_fs

- name: Debug monitor filesystem status
  debug:
    msg: |
      Monitor filesystem check:
      - Path: /var/lib/ceph/mon/ceph-{{ inventory_hostname }}
      - Exists: {{ mon_fs.stat.exists }}
      - Is directory: {{ mon_fs.stat.isdir | default(false) }}
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Fail if monitor initialization failed
  fail:
    msg: "Monitor filesystem was not created. Check the initialization task output above."
  when: not mon_fs.stat.exists and inventory_hostname == groups['ceph_monitors'][0]

- name: Display monitor filesystem status
  debug:
    msg: |
      Monitor filesystem for {{ inventory_hostname }}:
      - Path: /var/lib/ceph/mon/ceph-{{ inventory_hostname }}
      - Exists: {{ mon_fs.stat.exists }}
      - Owner: {{ mon_fs.stat.pw_name }}:{{ mon_fs.stat.gr_name }}
      - Mode: {{ mon_fs.stat.mode }}

- name: Reset ceph-mon service failed state
  shell: |
    systemctl reset-failed ceph-mon@{{ inventory_hostname }} 2>/dev/null || true
  become: yes
  changed_when: false

- name: Enable and start ceph-mon service
  systemd:
    name: "ceph-mon@{{ inventory_hostname }}"
    state: started
    enabled: yes
    daemon_reload: yes
  register: mon_service
  ignore_errors: yes

- name: Capture journalctl output if service failed
  shell: |
    journalctl -xeu ceph-mon@{{ inventory_hostname }} -n 50 --no-pager
  register: mon_journal
  changed_when: false
  when: mon_service.failed | default(false)

- name: Display journal error details
  debug:
    msg: "{{ mon_journal.stdout }}"
  when: mon_service.failed | default(false)

- name: Fail with detailed error
  fail:
    msg: |
      Failed to start ceph-mon service. See journal output above.
      Check:
      1. Monitor data directory: /var/lib/ceph/mon/ceph-{{ inventory_hostname }}
      2. Configuration: /etc/ceph/ceph.conf
      3. Keyring: /tmp/ceph.mon.keyring
  when: mon_service.failed | default(false)

- name: Wait for Ceph monitor to be ready
  shell: |
    for i in {1..60}; do
      if timeout 5 ceph -s 2>/dev/null | grep -q "mon:"; then
        echo "Ceph monitor is ready"
        exit 0
      fi
      echo "Attempt $i/60: Waiting for monitor to respond..."
      sleep 5
    done
    echo "Monitor not responding after 5 minutes, but continuing..."
  register: ceph_ready
  ignore_errors: yes
  changed_when: false

- name: Create RBD pool for Kubernetes
  shell: |
    ceph osd pool create kubernetes 128 128 --autoscale-mode on 2>/dev/null || true
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Enable RBD application on pool
  shell: |
    ceph osd pool application enable kubernetes rbd 2>/dev/null || true
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Create bootstrap-osd keyring directory
  file:
    path: /var/lib/ceph/bootstrap-osd
    state: directory
    mode: "0755"
    owner: ceph
    group: ceph
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Check if bootstrap-osd keyring is already valid
  shell: |
    [ -s /var/lib/ceph/bootstrap-osd/ceph.keyring ] && echo "valid" || echo "invalid"
  register: bootstrap_keyring_check
  changed_when: false
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Delete old bootstrap-osd key if it exists with wrong capabilities
  shell: |
    ceph auth del client.bootstrap-osd 2>/dev/null || true
  ignore_errors: yes
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - bootstrap_keyring_check.stdout | default('invalid') != 'valid'

- name: Generate bootstrap-osd keyring
  shell: |
    ceph auth get-or-create client.bootstrap-osd mon 'allow profile bootstrap-osd' osd 'allow *' > /var/lib/ceph/bootstrap-osd/ceph.keyring
  ignore_errors: yes
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - bootstrap_keyring_check.stdout | default('invalid') != 'valid'

- name: Fix bootstrap-osd keyring permissions
  file:
    path: /var/lib/ceph/bootstrap-osd/ceph.keyring
    mode: "0644"
    owner: ceph
    group: ceph
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]
- name: Display Ceph cluster status
  shell: |
    ceph -s 2>/dev/null || echo "Ceph cluster initializing..."
  register: ceph_status
  changed_when: false
  ignore_errors: yes

- name: Show Ceph status
  debug:
    msg: "{{ ceph_status.stdout }}"

- name: Bootstrap additional monitors (if any)
  block:
    - name: Wait for primary monitor to be ready
      pause:
        seconds: 15
      when:
        - ceph_conf_stat.stat.exists == false
        - inventory_hostname != groups['ceph_monitors'][0]

    - name: Get Ceph configuration from primary monitor
      slurp:
        src: /etc/ceph/ceph.conf
      register: ceph_conf_content
      delegate_to: "{{ groups['ceph_monitors'][0] }}"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]

    - name: Write Ceph configuration on secondary monitor
      copy:
        content: "{{ ceph_conf_content.content | b64decode }}"
        dest: /etc/ceph/ceph.conf
        mode: "0644"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]
        - ceph_conf_content is defined

    - name: Get monitor keyring from primary
      slurp:
        src: /tmp/ceph.mon.keyring
      register: ceph_mon_keyring_content
      delegate_to: "{{ groups['ceph_monitors'][0] }}"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]

    - name: Write monitor keyring on secondary monitor
      copy:
        content: "{{ ceph_mon_keyring_content.content | b64decode }}"
        dest: /tmp/ceph.mon.keyring
        mode: "0600"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]
        - ceph_mon_keyring_content is defined

    - name: Get monmap from primary monitor
      slurp:
        src: /tmp/monmap
      register: monmap_content
      delegate_to: "{{ groups['ceph_monitors'][0] }}"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]

    - name: Write monmap on secondary monitor
      copy:
        content: "{{ monmap_content.content | b64decode }}"
        dest: /tmp/monmap
        mode: "0600"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]
        - monmap_content is defined

    - name: Initialize secondary Ceph monitor
      shell: |
        ceph-mon --mkfs -i {{ inventory_hostname }} \
          --monmap /tmp/monmap \
          --keyring /tmp/ceph.mon.keyring
      args:
        creates: "/var/lib/ceph/mon/ceph-{{ inventory_hostname }}"
      when:
        - not ceph_conf_stat.stat.exists
        - inventory_hostname != groups['ceph_monitors'][0]
  when: groups['ceph_monitors'] | length > 1

# Ceph Dashboard Configuration
- name: Create manager data directory
  file:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}"
    state: directory
    mode: "0755"
    owner: ceph
    group: ceph
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Generate Ceph manager keyring using ceph-authtool
  shell: |
    ceph-authtool --create-keyring /var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring \
      --gen-key -n mgr.{{ inventory_hostname }} \
      --cap mon 'allow profile mgr' \
      --cap osd 'allow *' \
      --cap mds 'allow *'
  args:
    creates: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
  when: inventory_hostname == groups['ceph_monitors'][0]
  ignore_errors: yes

- name: Fix manager keyring permissions
  file:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
    mode: "0600"
    owner: ceph
    group: ceph
  when: inventory_hostname == groups['ceph_monitors'][0]
  ignore_errors: yes

- name: Check if mgr keyring was created
  stat:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
  register: mgr_keyring_stat
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Display mgr keyring status
  debug:
    msg: "MGR keyring exists: {{ mgr_keyring_stat.stat.exists }}"
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Check if mgr entity exists in monitor auth
  command: ceph auth get mgr.{{ inventory_hostname }}
  register: mgr_auth_check
  failed_when: false
  changed_when: false
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Create mgr entity on monitor if missing and write keyring
  shell: |
    ceph auth get-or-create mgr.{{ inventory_hostname }} \
      mon 'allow profile mgr' osd 'allow *' mds 'allow *' \
      -o /var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring
  register: mgr_auth
  become: yes
  changed_when: "'added' in mgr_auth.stdout.lower() or 'created' in mgr_auth.stdout.lower() or mgr_auth.rc == 0"
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - mgr_auth_check.rc != 0

- name: Retrieve mgr keyring from monitor if entity exists but local keyring missing
  shell: |
    ceph auth get mgr.{{ inventory_hostname }} -o /var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring
  register: mgr_key_get
  become: yes
  changed_when: mgr_key_get.rc == 0
  failed_when: false
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - mgr_auth_check.rc == 0
    - not mgr_keyring_stat.stat.exists

- name: Fix manager keyring permissions (ensure ownership)
  file:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
    mode: "0600"
    owner: ceph
    group: ceph
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Re-check if mgr keyring was created
  stat:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
  register: mgr_keyring_stat
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Fix manager keyring permissions (ensure ownership)
  file:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
    mode: "0600"
    owner: ceph
    group: ceph
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Re-check if mgr keyring was created
  stat:
    path: "/var/lib/ceph/mgr/ceph-{{ inventory_hostname }}/keyring"
  register: mgr_keyring_stat
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Reset ceph-mgr service failed state
  shell: |
    systemctl reset-failed ceph-mgr@{{ inventory_hostname }} 2>/dev/null || true
  become: yes
  changed_when: false
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Start Ceph Manager service
  systemd:
    name: "ceph-mgr@{{ inventory_hostname }}"
    state: restarted
    enabled: yes
    daemon_reload: yes
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - mgr_keyring_stat.stat.exists
  register: mgr_service
  ignore_errors: yes

- name: Wait for Ceph Manager to be ready
  pause:
    seconds: 10
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Wait for Ceph cluster to be fully initialized
  pause:
    seconds: 30
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Enable Ceph Dashboard module
  shell: |
    ceph mgr module enable dashboard
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Set Ceph Dashboard server port
  shell: |
    ceph config set mgr mgr/dashboard/server_port 7000
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Disable HTTPS for Dashboard (self-signed cert issues)
  shell: |
    ceph config set mgr mgr/dashboard/ssl false
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Generate random password for dashboard
  shell: |
    head -c 16 /dev/urandom | base64 | tr -d '/+=' | cut -c1-16
  register: dashboard_password
  changed_when: false
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Delete existing admin user if present
  shell: |
    ceph dashboard ac-user-delete admin 2>/dev/null || true
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Wait for dashboard to be ready
  pause:
    seconds: 2
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Create new dashboard admin user
  shell: |
    echo "{{ dashboard_password.stdout }}" | ceph dashboard ac-user-create admin administrator -i /dev/stdin
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Save dashboard password to file
  copy:
    content: "{{ dashboard_password.stdout }}"
    dest: /tmp/ceph-dashboard-admin.txt
    mode: "0600"
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Confirm dashboard setup
  debug:
    msg: "Dashboard admin user configured. Credentials saved to /tmp/ceph-dashboard-admin.txt"
  when: inventory_hostname == groups['ceph_monitors'][0]
  ignore_errors: yes
  register: dashboard_user
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Display dashboard user creation output
  debug:
    msg: "{{ dashboard_user.stdout }}"
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - dashboard_user.stdout is defined

- name: Get Dashboard admin password
  slurp:
    src: /tmp/ceph-dashboard-admin.txt
  register: dashboard_password
  ignore_errors: yes
  when: inventory_hostname == groups['ceph_monitors'][0]

- name: Display Ceph Dashboard access information
  debug:
    msg: |
      =====================================================
      Ceph Dashboard is now configured and ready to use
      =====================================================

      Dashboard URL: http://{{ hostvars[groups['ceph_monitors'][0]]['ansible_default_ipv4']['address'] }}:7000

      Default Username: admin
      Default Password: Check /tmp/ceph-dashboard-admin.txt on {{ groups['ceph_monitors'][0] }}

      To retrieve the password:
      ssh {{ hostvars[groups['ceph_monitors'][0]]['ansible_user'] }}@{{ hostvars[groups['ceph_monitors'][0]]['ansible_host'] }}
      cat /tmp/ceph-dashboard-admin.txt
      =====================================================
  when: inventory_hostname == groups['ceph_monitors'][0]
